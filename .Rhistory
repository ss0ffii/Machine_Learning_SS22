x <- owner2[,"ex_showroom_price"]
y <- owner2[,"selling_price"]
points(x,y,pch=19,col="red")
#################################################################
# Zusammenhang: metrisch-nominal
# Trennung der Daten nach Owner
owner1 <- subset(bikes,owner==1)
owner2 <- subset(bikes,owner==2)
# Herausgreifen des jeweiligen Verbrauchs
x <- owner1[,"year"]
y <- owner2[,"year"]
# Malen der Boxplots
boxplot(x,y,main="year",names=c("owner1","owner2"))
#################################################################
# Korrelationen
# Herausgreifen von selling_price, year und ex_showroom_price
x <- bikes[,"selling_price"]
y <- bikes[,"year"]
z <- bikes[,"ex_showroom_price"]
# Berechnung der Korrelationen
cor(x,y)
cor(x,z)
#####################################
#  k-Nearest Neighbors: Regression  #
#####################################
#################################################################
# Pakete
# Install die FNN Package für k-Nearest Neighbors -> install.packages("FNN")
library(FNN)
#################################################################
# Pfad setzen
setwd("D:/OneDrive - Technische Hochschule Deggendorf/Materialen - Uni/4. Semester/Maschinelles Lernen/Prüfungsstudienarbeit")
bikes <- read.csv(
"bikes_imputed.csv",
header=TRUE,
sep=",",
fill=TRUE,
stringsAsFactors=TRUE)
bikes[1:10,]
summary(bikes)
# Reduzierung der Kategorien / Zusammenfassen von Kategorien
bikes[,"owner"] <- ifelse(bikes[,"owner"] == "1st owner" | bikes[,"owner"] == "2nd owner", bikes[,"owner"], 2)
bikes[,"owner"] <- as.factor(bikes[,"owner"])
summary(bikes)
#################################################################
# Wir betrachten zunaechst nur den Zusammenhang zwischen selling_price und year
x <- bikes[,"year"]
y <- bikes[,"selling_price"]
plot(x,y,pch=19, xlab = "Jahr", ylab = "Preis")
#################################################################
# Aufteilung des Datensatzes in Trainings- und Validierungsdatensatz bzw. Testdatensatz
# Vertauschung der Reihenfolge der Daten
n <- length(bikes[,1])
n
index <- sample(1:n,n,replace=FALSE)
bikes <- bikes[index,]
# Aufteilung in Training/Validierung und Test
bikes.train.val <- bikes[1:743,]
bikes.test <- bikes[744:1061,]
# Anlage von Vektoren fuer die Guete
guete <- 0
guete.temp <- 0
# Start der for-Schleife fuer verschiedene Werte von k
for(k in 1:20){
# Start der for-Schleife fuer Leave-One-Out
m <- length(bikes.train.val[,1])
for(i in 1:m){
# Aufteilen in Trainingsdaten und Validierungsdatenpunkt
bikes.train <- bikes.train.val[-i,]
bikes.val <- bikes.train.val[i,]
# Berechnung von k-Nearest-Neighbors und der Prognoseguete
x.train <- bikes.train[,"year"]
y.train <- bikes.train[,"selling_price"]
x.val <- bikes.val[,"year"]
y.val <- bikes.val[,"selling_price"]
x.train <- matrix(x.train)
x.val <- matrix(x.val)
model <- knn.reg(train=x.train, test = x.val, y=y.train, k = k)
prognosen <- model$pred
guete.temp[i] <- abs(prognosen-y.val)
}
# Berechnung der durchschnittlichen Prognoseguete fuer k
guete[k] <- mean(guete.temp)
}
# Ausgabe der Gueten fuer die verschiedenen Werte von k
guete
plot(1:20,guete,pch=19, xlab = "Jahr", ylab = "Preis")
# Ausgabe des optimalen k mit minimalen Prognosefehler
which.min(guete)
# Berechnung des Modells auf dem Trainings- und Validierungsdatensatz fuer das optimale k
x.train <- bikes.train.val[,"year"]
y.train <- bikes.train.val[,"selling_price"]
# Testdaten
x.test <- bikes.test[,"year"]
y.test <- bikes.test[,"selling_price"]
# Weil x.train und x.test im Moment Vectoren sind, muessen diese zunaechst in
# Matrizen umgewandelt werden (das ist hier noetig, weil wir nur 1 Einflussvariable haben)
x.train <- matrix(x.train)
x.test <- matrix(x.test)
# Berechnung der Prognosen mit k-Nearest Neighbors fuer k=11 und des Prognosefehlers
model <- knn.reg(train=x.train, test = x.test, y=y.train, k = 1)
prognosen <- model$pred
mean(abs(prognosen-y.test))
#################################################################
# Maschinelles Lernen mit SVMs
#################################################################
# F?r maschinelles Lernen muss erst ein Zusatzpaket in R installiert werden
# Wir verwenden im folgenden das Paket "e1071"
# Pakete muss man (einmalig) mit folgendem Befehl installieren
# (Internetverbindung n?tig!):
# install.packages("e1071")
# Damit wird das Paket auf dem eigenen Computer installiert. Immer wenn
# man das Paket verwenden will, muss man es aber in der aktuellen
# R-Sitzung aus der Bibliothek laden. Das geht folgenderma?en:
library(e1071)
##################
# Regression mit SVR (Support Vector Regression)
##################
#Pfad setzen
setwd("C:/Users/darja/Downloads")
#ersetzt die Leerzeichen in den Daten durch R verstandliche "NA" Werte
# Damit wird das Paket auf dem eigenen Computer installiert. Immer wenn
# man das Paket verwenden will, muss man es aber in der aktuellen
# R-Sitzung aus der Bibliothek laden. Das geht folgenderma?en:
library(e1071)
##################
# Regression mit SVR (Support Vector Regression)
##################
#Pfad setzen
setwd("D:/OneDrive - Technische Hochschule Deggendorf/Materialen - Uni/4. Semester/Maschinelles Lernen/motorbikes_Gutoranska_Likhacheva_Solodovnyk")
#ersetzt die Leerzeichen in den Daten durch R verstandliche "NA" Werte
Daten <- read.csv("bikes_imputed.csv", header=TRUE, sep=",", fill=TRUE, stringsAsFactors=TRUE, na.strings=" ")
Daten[1:10,]
# Kontrolle der Datentypen durch Ausgabe der Summary
summary(Daten)
# Entfernung Ausrei?er
quartiles <- quantile(Daten$ex_showroom_price, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(Daten$ex_showroom_price)
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR
Daten <- subset(Daten, Daten$ex_showroom_price > Lower & Daten$ex_showroom_price < Upper)
quartiles <- quantile(Daten$km_driven, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(Daten$km_driven)
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR
Daten <- subset(Daten, Daten$km_driven > Lower & Daten$km_driven < Upper)
Daten[1:10,]
# Kontrolle der Datentypen durch Ausgabe der Summary
summary(Daten)
# Definition der Tuning-Parameter
cc <- seq(-5,10,1)       # f?r m?gliche Werte von "Cost" (Tuningparameter)
cg <- seq(-4,1,0.5)      # f?r m?gliche werte von "gamma" (Tuningparameter)
# Berechnung des Modells
tuning <- tune.svm(selling_price ~ name + year + km_driven + ex_showroom_price, data=Daten, scale = TRUE, type = "eps-regression", kernel = "radial", gamma = 10^cg, cost = 2^cc, epsilon = 0.1, tunecontrol = tune.control(sampling = "cross",cross=5))
print(tuning)
# Speichern des Modells mit den besten Parametern:
model <- tuning$best.model      # das Model mit optimalen Tuningparametern
# Berechnen von Prognosen
# Berechnung der Prognosen f?r die Motorr?der aus dem Datensatz:
# Auswahl aller Input-Variablen und Speicherung unter 'X'
X <- Daten[,c("name","year","km_driven","ex_showroom_price")]
# Berechnung der Prognosen
predict(model,X)
# Berechnung einer Prognose f?r einen neuen Datenpunkt
# Hinzuf?gen eines neuen Datenpunkts
x.neu <- data.frame("Honda CB Hornet 160R", 2018, 10000, 95000)
names(x.neu) <- names(X)
X <- rbind(X,x.neu)
# Berechnung der Prognosen
predict(model,X)
library(e1071)
##################
# Regression mit SVR (Support Vector Regression)
##################
#Pfad setzen
setwd("D:/OneDrive - Technische Hochschule Deggendorf/Materialen - Uni/4. Semester/Maschinelles Lernen/motorbikes_Gutoranska_Likhacheva_Solodovnyk")
#ersetzt die Leerzeichen in den Daten durch R verstandliche "NA" Werte
Daten <- read.csv("bikes_imputed.csv", header=TRUE, sep=",", fill=TRUE, stringsAsFactors=TRUE, na.strings=" ")
Daten[1:10,]
# Kontrolle der Datentypen durch Ausgabe der Summary
summary(Daten)
# Entfernung Ausrei?er
quartiles <- quantile(Daten$ex_showroom_price, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(Daten$ex_showroom_price)
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR
Daten <- subset(Daten, Daten$ex_showroom_price > Lower & Daten$ex_showroom_price < Upper)
quartiles <- quantile(Daten$km_driven, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(Daten$km_driven)
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR
Daten <- subset(Daten, Daten$km_driven > Lower & Daten$km_driven < Upper)
Daten[1:10,]
# Kontrolle der Datentypen durch Ausgabe der Summary
summary(Daten)
# Definition der Tuning-Parameter
cc <- seq(-5,10,1)       # f?r m?gliche Werte von "Cost" (Tuningparameter)
cg <- seq(-4,1,0.5)      # f?r m?gliche werte von "gamma" (Tuningparameter)
# Berechnung des Modells
tuning <- tune.svm(selling_price ~ name + year + km_driven + ex_showroom_price, data=Daten, scale = TRUE, type = "eps-regression", kernel = "radial", gamma = 10^cg, cost = 2^cc, epsilon = 0.1, tunecontrol = tune.control(sampling = "cross",cross=5))
setwd("D:/OneDrive - Technische Hochschule Deggendorf/Materialen - Uni/4. Semester/Maschinelles Lernen/motorbikes_Gutoranska_Likhacheva_Solodovnyk")
Data <- read.csv("bikes.csv",header=TRUE,sep=",",fill=TRUE,stringsAsFactors=TRUE)
# Explore and prepare dataset
Data[1:10,]
setwd("D:/OneDrive - Technische Hochschule Deggendorf/Materialen - Uni/4. Semester/Maschinelles Lernen/motorbikes_Gutoranska_Likhacheva_Solodovnyk")
Data <- read.csv("bikes_imputed.csv",header=TRUE,sep=",",fill=TRUE,stringsAsFactors=TRUE)
# Explore and prepare dataset
Data[1:10,]
Data[,"owner"] <- as.factor(Data[,"owner"])
Data[,"seller_type"] <- as.factor(Data[,"seller_type"])
Data[,"selling_price"] <- as.numeric(Data[,"selling_price"])
Data[,"year"] <- as.numeric(Data[,"year"])
Data[,"km_driven"] <- as.numeric(Data[,"km_driven"])
Data[,"ex_showroom_price"] <- as.numeric(Data[,"ex_showroom_price"])
bikes_with_outliers = Data[,c("owner", "seller_type", "year", "km_driven", "ex_showroom_price", "selling_price")]
summary(bikes_with_outliers)
# Remove outliers from dataset
quartiles <- quantile(bikes_with_outliers$year, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(bikes_with_outliers$year)
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR
year_outliers <- subset(bikes_with_outliers, bikes_with_outliers$year > Lower & bikes_with_outliers$year < Upper)
quartiles <- quantile(year_outliers$km_driven, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(year_outliers$km_driven)
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR
km_outliers <- subset(year_outliers, year_outliers$km_driven > Lower & year_outliers$km_driven < Upper)
quartiles <- quantile(km_outliers$ex_showroom_price, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(km_outliers$ex_showroom_price)
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR
bikes <- subset(km_outliers, km_outliers$ex_showroom_price > Lower & km_outliers$ex_showroom_price < Upper)
summary(bikes)
# Randomize the dataset and divide into training and test data
n <- length(bikes[,1])
Index <- sample(seq(1,n,1), replace=FALSE)
bikes <- bikes[Index,]
rownames(bikes) <- 1:n
nrow(bikes)
bikes.train <- bikes[1:710,]
bikes.test <- bikes[711:887,]
# Linear regression based on 'year' and 'km_driven'
model <- lm(selling_price ~ year + km_driven, data=bikes.train)
summary(model)
predicted <- predict(model, bikes.test)
actual <- bikes.test[,"selling_price"]
MAPE <- mean(abs((actual - predicted)/actual))*100
MAPE
# Linear regression based on 'year' and 'ex_showroom_price'
model <- lm(selling_price ~ year + ex_showroom_price, data=bikes.train)
summary(model)
predicted <- predict(model, bikes.test)
actual <- bikes.test[,"selling_price"]
MAPE <- mean(abs((actual - predicted)/actual))*100
MAPE
# Linear regression based on 'km_driven' and 'ex_showroom_price'
model <- lm(selling_price ~ km_driven + ex_showroom_price, data=bikes.train)
summary(model)
predicted <- predict(model, bikes.test)
actual <- bikes.test[,"selling_price"]
MAPE <- mean(abs((actual - predicted)/actual))*100
MAPE
# Linear regression based on 'year', 'km_driven' and 'ex_showroom_price'
model <- lm(selling_price ~ year + km_driven + ex_showroom_price, data=bikes.train)
summary(model)
predicted <- predict(model, bikes.test)
actual <- bikes.test[,"selling_price"]
MAPE <- mean(abs((actual - predicted)/actual))*100
MAPE
setwd("D:/OneDrive - Technische Hochschule Deggendorf/Materialen - Uni/4. Semester/Maschinelles Lernen/motorbikes_Gutoranska_Likhacheva_Solodovnyk")
Data <- read.csv("bikes_imputed.csv",header=TRUE,sep=",",fill=TRUE,stringsAsFactors=TRUE)
# Explore and prepare dataset
Data[1:10,]
Data[,"owner"] <- as.factor(Data[,"owner"])
Data[,"seller_type"] <- as.factor(Data[,"seller_type"])
Data[,"selling_price"] <- as.numeric(Data[,"selling_price"])
Data[,"year"] <- as.numeric(Data[,"year"])
Data[,"km_driven"] <- as.numeric(Data[,"km_driven"])
Data[,"ex_showroom_price"] <- as.numeric(Data[,"ex_showroom_price"])
bikes_with_outliers = Data[,c("owner", "seller_type", "year", "km_driven", "ex_showroom_price", "selling_price")]
summary(bikes_with_outliers)
# Remove outliers from dataset
quartiles <- quantile(bikes_with_outliers$year, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(bikes_with_outliers$year)
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR
year_outliers <- subset(bikes_with_outliers, bikes_with_outliers$year > Lower & bikes_with_outliers$year < Upper)
quartiles <- quantile(year_outliers$km_driven, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(year_outliers$km_driven)
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR
km_outliers <- subset(year_outliers, year_outliers$km_driven > Lower & year_outliers$km_driven < Upper)
quartiles <- quantile(km_outliers$ex_showroom_price, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(km_outliers$ex_showroom_price)
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR
bikes <- subset(km_outliers, km_outliers$ex_showroom_price > Lower & km_outliers$ex_showroom_price < Upper)
summary(bikes)
# Randomize the dataset and divide into training and test data
n <- length(bikes[,1])
Index <- sample(seq(1,n,1), replace=FALSE)
bikes <- bikes[Index,]
rownames(bikes) <- 1:n
nrow(bikes)
bikes.train <- bikes[1:710,]
bikes.test <- bikes[711:887,]
# Prepare package for LASSO
install.packages("glmnet")
library(glmnet)
#LASSO with automatic cross-validation
X <- model.matrix(selling_price ~. , bikes.train)
X <- X[,-1]
y <- bikes.train[,"selling_price"]
model.lasso <- cv.glmnet(X,y)
model.lasso <- cv.glmnet(X,y)
coef(model.lasso,s="lambda.1se")
coef(model.lasso,s="lambda.min")
# LASSO with 100 iterations
X <- model.matrix(selling_price ~. , bikes.train)
library(glmnet)
#LASSO with automatic cross-validation
X <- model.matrix(selling_price ~. , bikes.train)
X <- X[,-1]
y <- bikes.train[,"selling_price"]
model.lasso <- cv.glmnet(X,y)
model.lasso <- cv.glmnet(X,y)
coef(model.lasso,s="lambda.1se")
coef(model.lasso,s="lambda.min")
# LASSO with 100 iterations
X <- model.matrix(selling_price ~. , bikes.train)
X <- X[,-1]
y <- bikes.train[,"selling_price"]
m <- length(X[1,])
total.numbers <- rep(0,m)
RUNS <- 100
for( run in 1:RUNS ){
model.lasso <- cv.glmnet(X,y)
beta <- coef(model.lasso,s="lambda.1se")[-1,1]
total.numbers <- total.numbers + ifelse( beta != 0, 1, 0)
}
total.numbers <- as.matrix(total.numbers)
rownames(total.numbers) <- names(beta)
total.numbers
install.packages("mboost")
library(mboost)
#Pfad setzen
setwd("D:/OneDrive - Technische Hochschule Deggendorf/Materialen - Uni/4. Semester/Maschinelles Lernen/motorbikes_Gutoranska_Likhacheva_Solodovnyk")
#ersetzt die Leerzeichen in den Daten durch R verstandliche "NA" Werte
Daten <- read.csv("bikes_imputed.csv", header=TRUE, sep=",", fill=TRUE, stringsAsFactors=TRUE, na.strings=" ")
# Entfernung Ausrei?er
quartiles <- quantile(Daten$ex_showroom_price, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(Daten$ex_showroom_price)
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR
Daten <- subset(Daten, Daten$ex_showroom_price > Lower & Daten$ex_showroom_price < Upper)
quartiles <- quantile(Daten$km_driven, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(Daten$km_driven)
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR
Daten <- subset(Daten, Daten$km_driven > Lower & Daten$km_driven < Upper)
Daten[1:10,]
# Kontrolle der Datentypen durch Ausgabe der Summary
summary(Daten)
# Vertauschung der Reihenfolge der Daten
n<-length(Daten[,1])
n
index <- sample(1:n,n,replace=FALSE)
Daten <- Daten[index,]
# Aufteilung 910 Daten in Training und Test
Daten.train <- Daten[1:637,]
Daten.test <- Daten[638:910,]
# Berechnung des Modells (noch nicht kreuzvalidiert)
model <- gamboost(selling_price ~ year + km_driven + ex_showroom_price, data=Daten.train, dfbase = 4, control = boost_control(mstop = 1000))
model
par(mfrow=c(1,3)) # Weil es 3 Einflussvariablen sind
plot(model)
# Kreuzvalidierung:
cv10f <- cv(model.weights(model), type = "kfold")
cvm <- cvrisk(model, folds = cv10f, papply = lapply)
print(cvm)
mstop(cvm)
plot(cvm)
# Berechnung des Modells (nun kreuzvalidiert),
# daher steht in den Optionen: 'mstop = mstop(cvm)'
model <- gamboost(selling_price ~ year + km_driven + ex_showroom_price, data=Daten.train, dfbase = 4, control = boost_control(mstop = mstop(cvm)))
model
par(mfrow=c(1,3))  # Weil es 3 Einflussvariablen sind
plot(model)
# Berechnung der Prognoseergebnisse auf den Testdaten:
X.test <- Daten.test[,c("year" , "km_driven" , "ex_showroom_price")]
prognosen <- predict(model,X.test)
# Berechnung des mittleren Prognosefehlers (MAD)
y.test <- Daten.test[,"selling_price"]
mean(abs((y.test-prognosen)/y.test))*100
#Pfad setzen
setwd("D:/OneDrive - Technische Hochschule Deggendorf/Materialen - Uni/4. Semester/Maschinelles Lernen/motorbikes_Gutoranska_Likhacheva_Solodovnyk")
#ersetzt die Leerzeichen in den Daten durch R verstandliche "NA" Werte
Daten <- read.csv("bikes_imputed.csv", header=TRUE, sep=",", fill=TRUE, stringsAsFactors=TRUE, na.strings=" ")
Daten[1:10,]
# Kontrolle der Datentypen durch Ausgabe der Summary
summary(Daten)
# Entfernung Ausrei?er
quartiles <- quantile(Daten$ex_showroom_price, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(Daten$ex_showroom_price)
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR
Daten <- subset(Daten, Daten$ex_showroom_price > Lower & Daten$ex_showroom_price < Upper)
quartiles <- quantile(Daten$km_driven, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(Daten$km_driven)
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR
Daten <- subset(Daten, Daten$km_driven > Lower & Daten$km_driven < Upper)
Daten[1:10,]
# Kontrolle der Datentypen durch Ausgabe der Summary
summary(Daten)
# Definition der Tuning-Parameter
cc <- seq(-5,10,1)       # f?r m?gliche Werte von "Cost" (Tuningparameter)
cg <- seq(-4,1,0.5)      # f?r m?gliche werte von "gamma" (Tuningparameter)
# Berechnung des Modells
tuning <- tune.svm(selling_price ~ name + year + km_driven + ex_showroom_price, data=Daten, scale = TRUE, type = "eps-regression", kernel = "radial", gamma = 10^cg, cost = 2^cc, epsilon = 0.1, tunecontrol = tune.control(sampling = "cross",cross=5))
print(tuning)
# Speichern des Modells mit den besten Parametern:
model <- tuning$best.model      # das Model mit optimalen Tuningparametern
# Berechnen von Prognosen
# Berechnung der Prognosen f?r die Motorr?der aus dem Datensatz:
# Auswahl aller Input-Variablen und Speicherung unter 'X'
X <- Daten[,c("name","year","km_driven","ex_showroom_price")]
# Berechnung der Prognosen
predict(model,X)
# Berechnung einer Prognose f?r einen neuen Datenpunkt
# Hinzuf?gen eines neuen Datenpunkts
x.neu <- data.frame("Honda CB Hornet 160R", 2018, 10000, 95000)
names(x.neu) <- names(X)
X <- rbind(X,x.neu)
# Berechnung der Prognosen
predict(model,X)
#############################
# Nun wiederholen wir die Berechnung nochmal, wobei wir den Datensatz
# vorher in Test- und Trainingsdaten aufteilen:
# Aufteilen der Daten:
# Anzahl der Daten
length(Daten[,1])     # 910 (70% zu 30% Aufteilung - 637 Datenpunkte als Trainingsdaten)
Daten.train <- Daten[1:637,]
Daten.test <- Daten[638:910,]
library(e1071)
##################
# Regression mit SVR (Support Vector Regression)
##################
#Pfad setzen
setwd("D:/OneDrive - Technische Hochschule Deggendorf/Materialen - Uni/4. Semester/Maschinelles Lernen/motorbikes_Gutoranska_Likhacheva_Solodovnyk")
#ersetzt die Leerzeichen in den Daten durch R verstandliche "NA" Werte
Daten <- read.csv("bikes_imputed.csv", header=TRUE, sep=",", fill=TRUE, stringsAsFactors=TRUE, na.strings=" ")
Daten[1:10,]
# Kontrolle der Datentypen durch Ausgabe der Summary
summary(Daten)
# Entfernung Ausrei?er
quartiles <- quantile(Daten$ex_showroom_price, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(Daten$ex_showroom_price)
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR
Daten <- subset(Daten, Daten$ex_showroom_price > Lower & Daten$ex_showroom_price < Upper)
quartiles <- quantile(Daten$km_driven, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(Daten$km_driven)
Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR
Daten <- subset(Daten, Daten$km_driven > Lower & Daten$km_driven < Upper)
Daten[1:10,]
# Kontrolle der Datentypen durch Ausgabe der Summary
summary(Daten)
# Definition der Tuning-Parameter
cc <- seq(-5,10,1)       # f?r m?gliche Werte von "Cost" (Tuningparameter)
cg <- seq(-4,1,0.5)      # f?r m?gliche werte von "gamma" (Tuningparameter)
# Berechnung des Modells
tuning <- tune.svm(selling_price ~ name + year + km_driven + ex_showroom_price, data=Daten, scale = TRUE, type = "eps-regression", kernel = "radial", gamma = 10^cg, cost = 2^cc, epsilon = 0.1, tunecontrol = tune.control(sampling = "cross",cross=5))
print(tuning)
# Speichern des Modells mit den besten Parametern:
model <- tuning$best.model      # das Model mit optimalen Tuningparametern
# Berechnen von Prognosen
# Berechnung der Prognosen f?r die Motorr?der aus dem Datensatz:
# Auswahl aller Input-Variablen und Speicherung unter 'X'
X <- Daten[,c("name","year","km_driven","ex_showroom_price")]
# Berechnung der Prognosen
predict(model,X)
# Berechnung einer Prognose f?r einen neuen Datenpunkt
# Hinzuf?gen eines neuen Datenpunkts
x.neu <- data.frame("Honda CB Hornet 160R", 2018, 10000, 95000)
names(x.neu) <- names(X)
X <- rbind(X,x.neu)
# Berechnung der Prognosen
predict(model,X)
#############################
# Nun wiederholen wir die Berechnung nochmal, wobei wir den Datensatz
# vorher in Test- und Trainingsdaten aufteilen:
# Aufteilen der Daten:
# Anzahl der Daten
length(Daten[,1])     # 910 (70% zu 30% Aufteilung - 637 Datenpunkte als Trainingsdaten)
Daten.train <- Daten[1:637,]
Daten.test <- Daten[638:910,]
# Definition der Tuning-Parameter
cc <- seq(-5,10,1)         # f?r m?gliche Werte von "Cost" (Tuningparameter)
cg <- seq(-4,1,0.5)        # f?r m?gliche werte von "gamma" (Tuningparameter)
# Berechnung des Modells
tuning <- tune.svm(selling_price ~ name + year + km_driven + ex_showroom_price, data=Daten, scale = TRUE, type = "eps-regression", kernel = "radial", gamma = 10^cg, cost = 2^cc, epsilon = 0.1, tunecontrol = tune.control(sampling = "cross",cross=5))
print(tuning)
# Speichern des Modells mit den besten Parametern:
model <- tuning$best.model      # das Model mit optimalen Tuningparametern
# Berechnung der Prognoseergebnisse auf den Testdaten:
X.test <- Daten.test[,c("name", "year","km_driven","ex_showroom_price")]
prognosen <- predict(model,X.test)
# Berechnung des mittleren Prognosefehlers (MAD)
y.test <- Daten.test[,"selling_price"]
mean(abs((y.test-prognosen)/y.test))*100
